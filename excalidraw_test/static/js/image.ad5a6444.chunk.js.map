{"version":3,"sources":["data/encode.ts","data/image.ts"],"names":["toByteString","data","Promise","resolve","reject","blob","Blob","TextEncoder","encode","Uint8Array","reader","FileReader","onload","event","target","result","Error","readAsBinaryString","byteStringToArrayBuffer","byteString","buffer","ArrayBuffer","length","bufferView","i","len","charCodeAt","byteStringToString","TextDecoder","decode","stringToBase64","str","a","window","btoa","base64ToString","base64","isByteString","atob","text","compress","deflate","deflated","console","error","version","encoding","compressed","encoded","decoded","inflate","to","blobToArrayBuffer","arrayBuffer","readAsArrayBuffer","getTEXtChunk","decodePng","chunks","metadataChunk","find","chunk","name","tEXt","encodePngMetadata","metadata","MIME_TYPES","excalidraw","JSON","stringify","splice","encodePng","type","png","decodePngMetadata","keyword","encodedData","parse","EXPORT_DATA_TYPES","encodeSvgMetadata","decodeSvgMetadata","svg","includes","match","versionMatch","json"],"mappings":"uaAQaA,EAAe,SAC1BC,GAEA,OAAO,IAAIC,SAAQ,SAACC,EAASC,GAC3B,IAAMC,EACY,kBAATJ,EACH,IAAIK,KAAK,EAAC,IAAIC,aAAcC,OAAOP,KACnC,IAAIK,KAAK,CAACL,aAAgBQ,WAAaR,EAAO,IAAIQ,WAAWR,KAC7DS,EAAS,IAAIC,WACnBD,EAAOE,OAAS,SAACC,GACf,IAAKA,EAAMC,QAAyC,kBAAxBD,EAAMC,OAAOC,OACvC,OAAOX,EAAO,IAAIY,MAAM,oCAE1Bb,EAAQU,EAAMC,OAAOC,SAEvBL,EAAOO,mBAAmBZ,OAIxBa,EAA0B,SAACC,GAG/B,IAFA,IAAMC,EAAS,IAAIC,YAAYF,EAAWG,QACpCC,EAAa,IAAId,WAAWW,GACzBI,EAAI,EAAGC,EAAMN,EAAWG,OAAQE,EAAIC,EAAKD,IAChDD,EAAWC,GAAKL,EAAWO,WAAWF,GAExC,OAAOJ,GAGHO,EAAqB,SAACR,GAC1B,OAAO,IAAIS,YAAY,SAASC,OAAOX,EAAwBC,KAWpDW,EAAc,uCAAG,WAAOC,GAAP,yBAAAC,EAAA,iHACNC,OAAOC,KAAKH,GADN,mCACaE,OADb,SAC+BjC,EAAa+B,GAD5C,6BACoBG,KADpB,mGAAH,sDAKdC,EAAc,uCAAG,WAAOC,GAAP,2BAAAJ,EAAA,6DAAuBK,EAAvB,kDACrBA,EACHJ,OAAOK,KAAKF,GACZT,EAAmBM,OAAOK,KAAKF,KAHP,2CAAH,sDAsBd5B,EAAM,uCAAG,+BAAAwB,EAAA,yDACpBO,EADoB,EACpBA,MAQiB,IATG,EAEpBC,SAFoB,0CAWCxC,EAAayC,kBAAQF,IAXtB,OAWhBG,EAXgB,uDAahBC,QAAQC,MAAM,yBAAd,MAbgB,kBAmBJF,EAnBI,KAoBTA,EApBS,uCAoBU1C,EAAauC,GApBvB,gEAiBlBM,QAAS,IACTC,SAAU,UACVC,WAnBkB,KAoBlBC,QApBkB,gEAAH,sDAwBNnB,EAAM,uCAAG,WAAO5B,GAAP,eAAA+B,EAAA,2DAGZ/B,EAAK6C,SAHO,OAIb,YAJa,2BAMN7C,EAAK8C,WANC,qBAOZ9C,EAAK+C,QAPO,uCAQNrB,EAAmB1B,EAAK+C,SARlB,kCAMhBC,EANgB,wCAWV,IAAIjC,MAAJ,oCAAuCf,EAAK6C,SAA5C,MAXU,YAchB7C,EAAK8C,WAdW,0CAeXG,kBAAQ,IAAIzC,WAAWS,EAAwB+B,IAAW,CAC/DE,GAAI,YAhBY,iCAoBbF,GApBa,4CAAH,sDAsGnB,IC/LMG,EAAoB,SAAC/C,GACzB,MAAI,gBAAiBA,EACZA,EAAKgD,cAGP,IAAInD,SAAQ,SAACC,EAASC,GAC3B,IAAMM,EAAS,IAAIC,WACnBD,EAAOE,OAAS,SAACC,GAAW,IAAD,EACzB,KAAI,UAACA,EAAMC,cAAP,aAAC,EAAcC,QACjB,OAAOX,EAAO,IAAIY,MAAM,yCAE1Bb,EAAQU,EAAMC,OAAOC,SAEvBL,EAAO4C,kBAAkBjD,OAIhBkD,EAAY,uCAAG,WAC1BlD,GAD0B,iBAAA2B,EAAA,kEAGXwB,IAHW,KAGG/C,WAHH,SAGoB2C,EAAkB/C,GAHtC,0CAGpBoD,GAHoB,gBAIpBC,EAAgBD,EAAOE,MAAK,SAACC,GAAD,MAA0B,SAAfA,EAAMC,SAJzB,0CAMjBC,IAAKjC,OAAO6B,EAAczD,OANT,iCAQnB,MARmB,4CAAH,sDAWZ8D,EAAiB,uCAAG,mCAAA/B,EAAA,6DAC/B3B,EAD+B,EAC/BA,KACA2D,EAF+B,EAE/BA,SAF+B,KAOhBR,IAPgB,KAOF/C,WAPE,SAOe2C,EAAkB/C,GAPjC,8CAOzBoD,GAPyB,mBASTK,IATS,KAU7BG,IAAWC,WAVkB,KAW7BC,KAX6B,UAYrB3D,EAAO,CACX+B,KAAMyB,EACNxB,UAAU,IAde,qCAWxB4B,UAXwB,gBASzBV,EATyB,KASJlD,OATI,qBAmB/BiD,EAAOY,QAAQ,EAAG,EAAGX,GAnBU,kBAqBxB,IAAIpD,KAAK,CAACgE,IAAUb,IAAU,CAAEc,KAAMN,IAAWO,OArBzB,4CAAH,sDAwBjBC,EAAiB,uCAAG,WAAOpE,GAAP,iBAAA2B,EAAA,sEACRuB,EAAalD,GADL,WAEnB,QADN2D,EADyB,cAEnB,IAARA,OAAA,EAAAA,EAAUU,WAAYT,IAAWC,WAFN,6BAKrB,YADAS,EAAcR,KAAKS,MAAMZ,EAASzB,OAJb,sBAQvB,SAAUoC,IACVA,EAAYJ,OAASM,IAAkBX,WAThB,yCAWhBF,EAASzB,MAXO,aAanB,IAAIvB,MAAM,UAbS,yBAeda,EAAO8C,GAfO,iFAiB3BhC,QAAQC,MAAR,MACM,IAAI5B,MAAM,UAlBW,cAqBzB,IAAIA,MAAM,WArBe,0DAAH,sDA4BjB8D,EAAiB,uCAAG,iCAAA9C,EAAA,6DAASO,EAAT,EAASA,KAAT,KACVT,EADU,KAE7BqC,KAF6B,SAER3D,EAAO,CAAE+B,SAFD,oCAExB6B,UAFwB,wCAG7B,GAH6B,cACzBhC,EADyB,OAM3B4B,EAAW,GACfA,GAAQ,+BAAyBC,IAAWC,WAApC,WACRF,GAAQ,mCACRA,GAAY,+BACZA,GAAY5B,EACZ4B,GAAY,6BAXmB,kBAYxBA,GAZwB,4CAAH,sDAejBe,EAAiB,uCAAG,yCAAA/C,EAAA,2DAASgD,EAAT,EAASA,KAChCC,SAAJ,uBAA6BhB,IAAWC,aADb,oBAEvBgB,EAAQF,EAAIE,MAAM,mDAFK,sBAIrB,IAAIlE,MAAM,WAJW,cAMvBmE,EAAeH,EAAIE,MAAM,kCACzBrC,GAAsB,OAAZsC,QAAY,IAAZA,OAAA,EAAAA,EAAe,KAAM,IAC/B9C,EAA2B,MAAZQ,EARQ,mBAWRV,EAAe+C,EAAM,GAAI7C,GAXjB,WAWrB+C,EAXqB,OAarB,YADAT,EAAcR,KAAKS,MAAMQ,IAZJ,sBAgBvB,SAAUT,IACVA,EAAYJ,OAASM,IAAkBX,WAjBhB,0CAmBhBkB,GAnBgB,cAqBnB,IAAIpE,MAAM,UArBS,yBAuBda,EAAO8C,GAvBO,iFAyB3BhC,QAAQC,MAAR,MACM,IAAI5B,MAAM,UA1BW,cA6BzB,IAAIA,MAAM,WA7Be,0DAAH","file":"static/js/image.ad5a6444.chunk.js","sourcesContent":["import { deflate, inflate } from \"pako\";\nimport { encryptData, decryptData } from \"./encryption\";\n\n// -----------------------------------------------------------------------------\n// byte (binary) strings\n// -----------------------------------------------------------------------------\n\n// fast, Buffer-compatible implem\nexport const toByteString = (\n  data: string | Uint8Array | ArrayBuffer,\n): Promise<string> => {\n  return new Promise((resolve, reject) => {\n    const blob =\n      typeof data === \"string\"\n        ? new Blob([new TextEncoder().encode(data)])\n        : new Blob([data instanceof Uint8Array ? data : new Uint8Array(data)]);\n    const reader = new FileReader();\n    reader.onload = (event) => {\n      if (!event.target || typeof event.target.result !== \"string\") {\n        return reject(new Error(\"couldn't convert to byte string\"));\n      }\n      resolve(event.target.result);\n    };\n    reader.readAsBinaryString(blob);\n  });\n};\n\nconst byteStringToArrayBuffer = (byteString: string) => {\n  const buffer = new ArrayBuffer(byteString.length);\n  const bufferView = new Uint8Array(buffer);\n  for (let i = 0, len = byteString.length; i < len; i++) {\n    bufferView[i] = byteString.charCodeAt(i);\n  }\n  return buffer;\n};\n\nconst byteStringToString = (byteString: string) => {\n  return new TextDecoder(\"utf-8\").decode(byteStringToArrayBuffer(byteString));\n};\n\n// -----------------------------------------------------------------------------\n// base64\n// -----------------------------------------------------------------------------\n\n/**\n * @param isByteString set to true if already byte string to prevent bloat\n *  due to reencoding\n */\nexport const stringToBase64 = async (str: string, isByteString = false) => {\n  return isByteString ? window.btoa(str) : window.btoa(await toByteString(str));\n};\n\n// async to align with stringToBase64\nexport const base64ToString = async (base64: string, isByteString = false) => {\n  return isByteString\n    ? window.atob(base64)\n    : byteStringToString(window.atob(base64));\n};\n\n// -----------------------------------------------------------------------------\n// text encoding\n// -----------------------------------------------------------------------------\n\ntype EncodedData = {\n  encoded: string;\n  encoding: \"bstring\";\n  /** whether text is compressed (zlib) */\n  compressed: boolean;\n  /** version for potential migration purposes */\n  version?: string;\n};\n\n/**\n * Encodes (and potentially compresses via zlib) text to byte string\n */\nexport const encode = async ({\n  text,\n  compress,\n}: {\n  text: string;\n  /** defaults to `true`. If compression fails, falls back to bstring alone. */\n  compress?: boolean;\n}): Promise<EncodedData> => {\n  let deflated!: string;\n  if (compress !== false) {\n    try {\n      deflated = await toByteString(deflate(text));\n    } catch (error: any) {\n      console.error(\"encode: cannot deflate\", error);\n    }\n  }\n  return {\n    version: \"1\",\n    encoding: \"bstring\",\n    compressed: !!deflated,\n    encoded: deflated || (await toByteString(text)),\n  };\n};\n\nexport const decode = async (data: EncodedData): Promise<string> => {\n  let decoded: string;\n\n  switch (data.encoding) {\n    case \"bstring\":\n      // if compressed, do not double decode the bstring\n      decoded = data.compressed\n        ? data.encoded\n        : await byteStringToString(data.encoded);\n      break;\n    default:\n      throw new Error(`decode: unknown encoding \"${data.encoding}\"`);\n  }\n\n  if (data.compressed) {\n    return inflate(new Uint8Array(byteStringToArrayBuffer(decoded)), {\n      to: \"string\",\n    });\n  }\n\n  return decoded;\n};\n\n// -----------------------------------------------------------------------------\n// binary encoding\n// -----------------------------------------------------------------------------\n\ntype FileEncodingInfo = {\n  /* version 2 is the version we're shipping the initial image support with.\n    version 1 was a PR version that a lot of people were using anyway.\n    Thus, if there are issues we can check whether they're not using the\n    unoffic version */\n  version: 1 | 2;\n  compression: \"pako@1\" | null;\n  encryption: \"AES-GCM\" | null;\n};\n\n// -----------------------------------------------------------------------------\nconst CONCAT_BUFFERS_VERSION = 1;\n/** how many bytes we use to encode how many bytes the next chunk has.\n * Corresponds to DataView setter methods (setUint32, setUint16, etc).\n *\n * NOTE ! values must not be changed, which would be backwards incompatible !\n */\nconst VERSION_DATAVIEW_BYTES = 4;\nconst NEXT_CHUNK_SIZE_DATAVIEW_BYTES = 4;\n// -----------------------------------------------------------------------------\n\nconst DATA_VIEW_BITS_MAP = { 1: 8, 2: 16, 4: 32 } as const;\n\n// getter\nfunction dataView(buffer: Uint8Array, bytes: 1 | 2 | 4, offset: number): number;\n// setter\nfunction dataView(\n  buffer: Uint8Array,\n  bytes: 1 | 2 | 4,\n  offset: number,\n  value: number,\n): Uint8Array;\n/**\n * abstraction over DataView that serves as a typed getter/setter in case\n * you're using constants for the byte size and want to ensure there's no\n * discrepenancy in the encoding across refactors.\n *\n * DataView serves for an endian-agnostic handling of numbers in ArrayBuffers.\n */\nfunction dataView(\n  buffer: Uint8Array,\n  bytes: 1 | 2 | 4,\n  offset: number,\n  value?: number,\n): Uint8Array | number {\n  if (value != null) {\n    if (value > Math.pow(2, DATA_VIEW_BITS_MAP[bytes]) - 1) {\n      throw new Error(\n        `attempting to set value higher than the allocated bytes (value: ${value}, bytes: ${bytes})`,\n      );\n    }\n    const method = `setUint${DATA_VIEW_BITS_MAP[bytes]}` as const;\n    new DataView(buffer.buffer)[method](offset, value);\n    return buffer;\n  }\n  const method = `getUint${DATA_VIEW_BITS_MAP[bytes]}` as const;\n  return new DataView(buffer.buffer)[method](offset);\n}\n\n// -----------------------------------------------------------------------------\n\n/**\n * Resulting concatenated buffer has this format:\n *\n * [\n *   VERSION chunk (4 bytes)\n *   LENGTH chunk 1 (4 bytes)\n *   DATA chunk 1 (up to 2^32 bits)\n *   LENGTH chunk 2 (4 bytes)\n *   DATA chunk 2 (up to 2^32 bits)\n *   ...\n * ]\n *\n * @param buffers each buffer (chunk) must be at most 2^32 bits large (~4GB)\n */\nconst concatBuffers = (...buffers: Uint8Array[]) => {\n  const bufferView = new Uint8Array(\n    VERSION_DATAVIEW_BYTES +\n      NEXT_CHUNK_SIZE_DATAVIEW_BYTES * buffers.length +\n      buffers.reduce((acc, buffer) => acc + buffer.byteLength, 0),\n  );\n\n  let cursor = 0;\n\n  // as the first chunk we'll encode the version for backwards compatibility\n  dataView(bufferView, VERSION_DATAVIEW_BYTES, cursor, CONCAT_BUFFERS_VERSION);\n  cursor += VERSION_DATAVIEW_BYTES;\n\n  for (const buffer of buffers) {\n    dataView(\n      bufferView,\n      NEXT_CHUNK_SIZE_DATAVIEW_BYTES,\n      cursor,\n      buffer.byteLength,\n    );\n    cursor += NEXT_CHUNK_SIZE_DATAVIEW_BYTES;\n\n    bufferView.set(buffer, cursor);\n    cursor += buffer.byteLength;\n  }\n\n  return bufferView;\n};\n\n/** can only be used on buffers created via `concatBuffers()` */\nconst splitBuffers = (concatenatedBuffer: Uint8Array) => {\n  const buffers = [];\n\n  let cursor = 0;\n\n  // first chunk is the version\n  const version = dataView(\n    concatenatedBuffer,\n    NEXT_CHUNK_SIZE_DATAVIEW_BYTES,\n    cursor,\n  );\n  // If version is outside of the supported versions, throw an error.\n  // This usually means the buffer wasn't encoded using this API, so we'd only\n  // waste compute.\n  if (version > CONCAT_BUFFERS_VERSION) {\n    throw new Error(`invalid version ${version}`);\n  }\n\n  cursor += VERSION_DATAVIEW_BYTES;\n\n  while (true) {\n    const chunkSize = dataView(\n      concatenatedBuffer,\n      NEXT_CHUNK_SIZE_DATAVIEW_BYTES,\n      cursor,\n    );\n    cursor += NEXT_CHUNK_SIZE_DATAVIEW_BYTES;\n\n    buffers.push(concatenatedBuffer.slice(cursor, cursor + chunkSize));\n    cursor += chunkSize;\n    if (cursor >= concatenatedBuffer.byteLength) {\n      break;\n    }\n  }\n\n  return buffers;\n};\n\n// helpers for (de)compressing data with JSON metadata including encryption\n// -----------------------------------------------------------------------------\n\n/** @private */\nconst _encryptAndCompress = async (\n  data: Uint8Array | string,\n  encryptionKey: string,\n) => {\n  const { encryptedBuffer, iv } = await encryptData(\n    encryptionKey,\n    deflate(data),\n  );\n\n  return { iv, buffer: new Uint8Array(encryptedBuffer) };\n};\n\n/**\n * The returned buffer has following format:\n * `[]` refers to a buffers wrapper (see `concatBuffers`)\n *\n * [\n *   encodingMetadataBuffer,\n *   iv,\n *   [\n *      contentsMetadataBuffer\n *      contentsBuffer\n *   ]\n * ]\n */\nexport const compressData = async <T extends Record<string, any> = never>(\n  dataBuffer: Uint8Array,\n  options: {\n    encryptionKey: string;\n  } & ([T] extends [never]\n    ? {\n        metadata?: T;\n      }\n    : {\n        metadata: T;\n      }),\n): Promise<Uint8Array> => {\n  const fileInfo: FileEncodingInfo = {\n    version: 2,\n    compression: \"pako@1\",\n    encryption: \"AES-GCM\",\n  };\n\n  const encodingMetadataBuffer = new TextEncoder().encode(\n    JSON.stringify(fileInfo),\n  );\n\n  const contentsMetadataBuffer = new TextEncoder().encode(\n    JSON.stringify(options.metadata || null),\n  );\n\n  const { iv, buffer } = await _encryptAndCompress(\n    concatBuffers(contentsMetadataBuffer, dataBuffer),\n    options.encryptionKey,\n  );\n\n  return concatBuffers(encodingMetadataBuffer, iv, buffer);\n};\n\n/** @private */\nconst _decryptAndDecompress = async (\n  iv: Uint8Array,\n  decryptedBuffer: Uint8Array,\n  decryptionKey: string,\n  isCompressed: boolean,\n) => {\n  decryptedBuffer = new Uint8Array(\n    await decryptData(iv, decryptedBuffer, decryptionKey),\n  );\n\n  if (isCompressed) {\n    return inflate(decryptedBuffer);\n  }\n\n  return decryptedBuffer;\n};\n\nexport const decompressData = async <T extends Record<string, any>>(\n  bufferView: Uint8Array,\n  options: { decryptionKey: string },\n) => {\n  // first chunk is encoding metadata (ignored for now)\n  const [encodingMetadataBuffer, iv, buffer] = splitBuffers(bufferView);\n\n  const encodingMetadata: FileEncodingInfo = JSON.parse(\n    new TextDecoder().decode(encodingMetadataBuffer),\n  );\n\n  try {\n    const [contentsMetadataBuffer, contentsBuffer] = splitBuffers(\n      await _decryptAndDecompress(\n        iv,\n        buffer,\n        options.decryptionKey,\n        !!encodingMetadata.compression,\n      ),\n    );\n\n    const metadata = JSON.parse(\n      new TextDecoder().decode(contentsMetadataBuffer),\n    ) as T;\n\n    return {\n      /** metadata source is always JSON so we can decode it here */\n      metadata,\n      /** data can be anything so the caller must decode it */\n      data: contentsBuffer,\n    };\n  } catch (error: any) {\n    console.error(\n      `Error during decompressing and decrypting the file.`,\n      encodingMetadata,\n    );\n    throw error;\n  }\n};\n\n// -----------------------------------------------------------------------------\n","import decodePng from \"png-chunks-extract\";\nimport tEXt from \"png-chunk-text\";\nimport encodePng from \"png-chunks-encode\";\nimport { stringToBase64, encode, decode, base64ToString } from \"./encode\";\nimport { EXPORT_DATA_TYPES, MIME_TYPES } from \"../constants\";\n\n// -----------------------------------------------------------------------------\n// PNG\n// -----------------------------------------------------------------------------\n\nconst blobToArrayBuffer = (blob: Blob): Promise<ArrayBuffer> => {\n  if (\"arrayBuffer\" in blob) {\n    return blob.arrayBuffer();\n  }\n  // Safari\n  return new Promise((resolve, reject) => {\n    const reader = new FileReader();\n    reader.onload = (event) => {\n      if (!event.target?.result) {\n        return reject(new Error(\"couldn't convert blob to ArrayBuffer\"));\n      }\n      resolve(event.target.result as ArrayBuffer);\n    };\n    reader.readAsArrayBuffer(blob);\n  });\n};\n\nexport const getTEXtChunk = async (\n  blob: Blob,\n): Promise<{ keyword: string; text: string } | null> => {\n  const chunks = decodePng(new Uint8Array(await blobToArrayBuffer(blob)));\n  const metadataChunk = chunks.find((chunk) => chunk.name === \"tEXt\");\n  if (metadataChunk) {\n    return tEXt.decode(metadataChunk.data);\n  }\n  return null;\n};\n\nexport const encodePngMetadata = async ({\n  blob,\n  metadata,\n}: {\n  blob: Blob;\n  metadata: string;\n}) => {\n  const chunks = decodePng(new Uint8Array(await blobToArrayBuffer(blob)));\n\n  const metadataChunk = tEXt.encode(\n    MIME_TYPES.excalidraw,\n    JSON.stringify(\n      await encode({\n        text: metadata,\n        compress: true,\n      }),\n    ),\n  );\n  // insert metadata before last chunk (iEND)\n  chunks.splice(-1, 0, metadataChunk);\n\n  return new Blob([encodePng(chunks)], { type: MIME_TYPES.png });\n};\n\nexport const decodePngMetadata = async (blob: Blob) => {\n  const metadata = await getTEXtChunk(blob);\n  if (metadata?.keyword === MIME_TYPES.excalidraw) {\n    try {\n      const encodedData = JSON.parse(metadata.text);\n      if (!(\"encoded\" in encodedData)) {\n        // legacy, un-encoded scene JSON\n        if (\n          \"type\" in encodedData &&\n          encodedData.type === EXPORT_DATA_TYPES.excalidraw\n        ) {\n          return metadata.text;\n        }\n        throw new Error(\"FAILED\");\n      }\n      return await decode(encodedData);\n    } catch (error: any) {\n      console.error(error);\n      throw new Error(\"FAILED\");\n    }\n  }\n  throw new Error(\"INVALID\");\n};\n\n// -----------------------------------------------------------------------------\n// SVG\n// -----------------------------------------------------------------------------\n\nexport const encodeSvgMetadata = async ({ text }: { text: string }) => {\n  const base64 = await stringToBase64(\n    JSON.stringify(await encode({ text })),\n    true /* is already byte string */,\n  );\n\n  let metadata = \"\";\n  metadata += `<!-- payload-type:${MIME_TYPES.excalidraw} -->`;\n  metadata += `<!-- payload-version:2 -->`;\n  metadata += \"<!-- payload-start -->\";\n  metadata += base64;\n  metadata += \"<!-- payload-end -->\";\n  return metadata;\n};\n\nexport const decodeSvgMetadata = async ({ svg }: { svg: string }) => {\n  if (svg.includes(`payload-type:${MIME_TYPES.excalidraw}`)) {\n    const match = svg.match(/<!-- payload-start -->(.+?)<!-- payload-end -->/);\n    if (!match) {\n      throw new Error(\"INVALID\");\n    }\n    const versionMatch = svg.match(/<!-- payload-version:(\\d+) -->/);\n    const version = versionMatch?.[1] || \"1\";\n    const isByteString = version !== \"1\";\n\n    try {\n      const json = await base64ToString(match[1], isByteString);\n      const encodedData = JSON.parse(json);\n      if (!(\"encoded\" in encodedData)) {\n        // legacy, un-encoded scene JSON\n        if (\n          \"type\" in encodedData &&\n          encodedData.type === EXPORT_DATA_TYPES.excalidraw\n        ) {\n          return json;\n        }\n        throw new Error(\"FAILED\");\n      }\n      return await decode(encodedData);\n    } catch (error: any) {\n      console.error(error);\n      throw new Error(\"FAILED\");\n    }\n  }\n  throw new Error(\"INVALID\");\n};\n"],"sourceRoot":""}